# LLM-ACT-R
This ongoing project aims to develop a novel hybrid architecture called LLM-ACTR designed to address the limitations of Large Language Models (LLMs) in decision-making.

## Structure

The repository is organized into the following main directories:

- **`/VSM-ACTR`**: This directory hosts all resources related to the VSM-ACTR cognitive model.
- **`/Data-Science-Notebooks`**: Contains notebooks for finetuning, training, evaluating, and visualizing data and model outputs.
- **`/docs`**: Documentation related to the project, including setup, usage, and examples.

### VSM-ACTR

- **`/model`**: Core model file.
- **`/raw data`**: output sample from cognitive model.
- **`/processed data`**: preprocessed dataset

### Data Science Notebooks

- **`/feature extraction`**: Notebooks for feature extraction for behavior prediction.
- **`/finetuning`**: Notebooks for fine-tuning for knowledge transfer.
- **`/baseline`**: Notebooks with two baselines.

## Getting Started

### Prerequisites

Before you can run the notebooks and scripts, you'll need to install several dependencies:

```bash
pip install -r requirements.txt
## Cite
```bibtex
@inproceedings{wu2024llama,
  title={LLAMA-ACT-R, a Neuro-Symbolic Architecture (ACT-R) for LLM Decision Making},
  author={Wu, Siyu and Giles, C. Lee and Ritter, Frank E.},
  booktitle={Poster Presented In Annual Ethical AI Symposium},
  year={2024},
  organization={University of Michigan Institute for Data Science}
}
```
